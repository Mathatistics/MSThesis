% !Rnw root = ../Main.Rnw

\chapter{Models and Methods}
\label{chap:modelsmethods}

\section{A statistical Model}
\label{sec:statModel}

A statistical model describes the relationship between a cause and its effect. Let a vector $\mbf{y}$ contains $n$ number of responses and $\mbf{X}$ be a $n\times p$ matrix whose columns are predictor variables and each of them have $n$ observations. These variables in $\mbf{X}$ can affect $\mbf{y}$ so, the relationship between $\mbf{X}$ and $\mbf{y}$ can be written in a functional form as, 

\begin{equation}
  \label{eq:functionalForm}
  \mbf{y}=f(\mbf{X})+\mbf{\epsilon}
\end{equation}

where, $\mbf{\epsilon}$ is a vector of unknown errors usually referred as `white noise' when dealing with time-series data which is assumed to have zero mean, constant variance and no autocorrelation.

\section{Linear Regression Model}
\label{sec:linRegModel}

The linear regression model with a single response $(\mbf{Y}=y_{t1}, y_{t2},\ldots, y_{tp})$ and $p$ predictor variable $\vl{X}{p}$ has form,

\begin{equation}
    \underbracket[0.6pt][5pt]{\mbf{Y}}_{\text{Response}}=\underbracket[0.5pt][5pt]{\beta_0+\beta_1X_{t1}+\beta_2X_{t2}+\ldots+\beta_pX_{tp}}_{\text{Mean Response explained by predictors only}}+\underbracket[0.6pt][5pt]{\epsilon}_{\text{Error Term}}
    \label{eq:linearRegModel}
\end{equation}

The model - \ref{eq:linearRegModel} is linear function of $p+1$ unknown parameters $\beta_\circ, \vl{\beta}{p}$ which is generally referred as regression coefficients. In matrix notation, equation- (\ref{eq:linearRegModel}) becomes,

\begin{equation}
  \label{eq:linRegMatModel}
  \underset{n\times 1}{\mbf{Y}}=\underset{n\times(p+1)}{\mbf{X}}\underset{(p+1)\times 1}{\bs{\beta}}+\underset{n\times 1}{\bs{\epsilon}}
\end{equation}

\subsection{Least Square Estimation}
\label{ssec:lstSqEstimation}

The estimate of the unknown parameter vector $\bs{\beta}$ in (\ref{eq:linRegMatModel}) is obtained by minimizing the sum of square of residuals, 
The sum of square of residuals is,

\begin{equation}
  \bs{\epsilon}^{t}\bs{\epsilon}=(\mbf{Y-X}\bs{\beta})^{t}(\mbf{Y-X}\bs{\beta})
  \label{eq:errorSq}
\end{equation}

On minimizing equation - \ref{eq:errorSq}, we get the OLS estimate of $\bs{\beta}$ as,

\begin{equation}
  \hat{\bs{\beta}}_{OLS}=(\mbf{X}^{t}\mbf{X})^{-1}\mbf{X}^{t}\mbf{Y}
  \label{eq:estOLS}
\end{equation}

For ordinary least square estimation, following basic assumptions (\cite{wooldridge2012introductory}) are required,
\begin{enumerate}
\item Linear in parameter \label{ass1}
\item Absence of Multicollinearity \label{ass2}
\item No correlation between Error terms and predictor variable, mathematically,
\[
E(\mbf{\epsilon}_i|\mbf{X})=0, t=1,2,\ldots, n
\]
The equation implies that the error term at time $t$ should be uncorrelated with each explanatory variable in every time period
\item Homoskedastic Error terms, i.e, \label{ass3}
\[
\var{\bs{\epsilon}_t|\mbf{X}}=\var{\mbf{\epsilon}_t}=\sigma^2\mbf{I}
\]
\item No serial correlation (autocorrelation) in error terms, i.e, \label{ass4}
\[
\corr{\bs{\epsilon}_t, \mbf{\epsilon}_s}=0, \forall t\ne s
\]
\end{enumerate}
For Hypothesis testing and inference using $t$ and $F$ test, an additional assumption of normality is needed, i.e
\[
\mbf{\epsilon}_t\sim N(\mbf{0}, \sigma^2\mbf{I})
\]
Under the assumption from \ref{ass1} to \ref{ass4}, the OLS estimate obtained from equation-\ref{eq:estOLS} is best linear unbiased estimator (BLUE) of $\beta$.

\subsection{Prediction}
\label{ssec:pred}

Using $\hat{\bs{\beta}}$ obtained in equation-\ref{eq:estOLS}, following two matrices can be obtained,

\begin{subequations}
\begin{align}
  \text{Predicted Values:}\hat{\mbf{Y}}   &=\mbf{X}\hat{\bs{\beta}}=\mbf{X(X^tX)^{-1}X^tY} \label{eq:predEquation}\\
  \text{Residuals:} \hat{\bs{\epsilon}}&=\mbf{Y}-\hat{\mbf{Y}} =[\mbf{I-X(X^tX)^{-1}X^t}]\mbf{Y} \label{eq:OLSResid}
\end{align}
\end{subequations}

Here equation-\ref{eq:predEquation} gives predicted values of $\mbf{Y}$ which on subtracting from observed value give the predicted error terms as is presented in equation-\ref{eq:OLSResid}. Equation-\ref{eq:predEquation} can also be written as, 

\begin{equation}
  \hat{\mbf{Y}} =\mbf{X}\hat{\bs{\beta}}=\mbf{HY} \label{eq:predHatEquation}
\end{equation}

Here, $\mbf{H}$ is called Hat matrix and is the orthogonal projection of $y$ onto the space spanned by the columns of $\mbf{X}$.

\section{Variable selection}
\label{sec:varSelection}
Although including many variables in the model can add information, they are also the source of unnecessary noise. In addition, many variables in a model is also the cause of multicollinearity. So, a model that is simple yet contain useful information is always desirable. Variable selection is intended for selecting best subset of predictor variables. Some of the criteria for variable selection as described in \citetitle{weisberg2005applied} by \citet{weisberg2005applied} are discussed below:

\subsection{Criteria for variable selection}
\label{ssec:vsCriteria}
Suppose $X_s$ is selected set of variable which gives the predicted output of,
\begin{equation}
\label{eq:selectedVarModel}
\hat{Y}=E\left(Y|X_s-x_s\right)=\beta'_s x_s
\end{equation}
If $X_s$ misses important variables, the residual sum of squares of fitted model in equation-\ref{eq:selectedVarModel} will be larger than the full model. Lack of fit for selecting the set $X_s$ is measured by its Error sum of square.
\begin{description}
\item[Model statistic Approach]\hfill\\
When a model is fitted, various statistics such as $R^2$, $R^2$-adj, F-statistic are obtained which measures the quality of that model. Based on these statistic, a model is selected as better than others.
\item[Information Criteria \label{itm:infCriteria}]\hfill \\
Another common criterion, which balances the size of the residual sum of squares with the number of parameters in the model (\cite[p.~386]{johnson2007applied}), for selecting subset of predictor variable is AIC (\textit{Akaike Information Criterion}). It is given as,
\begin{equation}
\label{eq:aicFormula}
\text{AIC} = n \log(\text{RSS}_s/n) + \text{k}
\end{equation}
where, RSS$=$Residual Sum of Square, $n=$number of observation and $k=$Number of variables included in the model

A model with smaller value of AIC obtained from equation-\ref{eq:aicFormula} is better better than other with larger AIC. An alternative to AIC is its Bayesian analogue, also known as Schwarz or Bayesian information criteria. Bayesian Information Criteria provides balance between model complexity and lack of fit. Smaller value of BIC is better.
\begin{equation}
\label{eq:bicFormula}
\text{BIC} = n \log(\text{RSS}_s/n) + \text{k} \log(n)
\end{equation}
A third criterion that balances the complexity and lack of fit of a model is Mallows’ $C_p$ (\cite{mallows1973some}), where the subscript $p$ is the number of variables in the candidate model. The formula for this statistic is given in equation-\ref{eq:malCp},
\begin{equation}
\label{eq:malCp}
\text{Mallows } C_p= \frac{\text{RSS}}{\hat{\sigma}^2} + 2k − n
\end{equation}
Where, $\hat{\sigma}^2$ is from the full model. A plot of $C_p$ vs $k$ for each subset of predictors indicate models that predict the responses well. Better models usually lie near the $45^\circ$ line of the plot.
\end{description}

\subsection{Computational procedure for variable selection}
\label{ssec:compProcVarSelction}
When a model is large, fitting all possible subsets is not feasible. \citet{furnival1974regressions} suggested several algorithm to calculate residual sum of square of all possible regression called leap and bound technique which has been widely implemented in statistical software. However, this method is not appropriate for criteria based on model statistic where step wise methods can be used.  methods has three basic variation (\cite[p.~221]{weisberg2005applied}).

\begin{description}
\item[Forward selection procedure \label{itm:forwardSelection}] \hfill\\
Model is started without any variable and in each step a variable is added and the model is fitted. The variable is left in the model if the subset minimizes the criterion of interest . Similar process is repeated for other predictor variables.

\item[Backward elimination procedure \label{itm:backwardElimination}]\hfill\\
This process is like the reverse of \hyperref[itm:forwardSelection]{Forward selection procedure}. In this process, the model is fitted with all the predictor variable and variables are removed one at a time except those that are forced to be in the model. The model is examined against the considered criteria. Usually, the term with smallest t-value is removed since this gives rise to the residual sum of square.

\item[Stepwise procedure \label{itm:stepwise}]\hfill\\
This combines both \hyperref[itm:forwardSelection]{Forward selection procedure} and \hyperref[itm:backwardElimination]{Backward elimination procedure}. In each step, a predictor variable is either deleted or added so that resulting model minimizes the criterion function of interest.

\end{description}

\section{Principal Component Analysis}
\label{sec:pca}

The purpose of PCA is to express the information in $\bs{X}=(\vl{X}{p})$ by a less number of variables $\mbf{Z}=(\vl{Z}{q}); q<p$ called principal components of \mbf{X} (\cite{martens1992multivariate}). These principal components are orthogonal and linearly uncorrelated. Since they are computed from the linear combinations of $\bs{X}$ variables, the variation in \mbf{X} variables are compressed in first few principal components. In other words, the first principal components is the direction along which the $\bs{X}$ variables have the largest variance (\cite{massart1998handbook}). In this situation, the multicollinearity in $\bs{X}$ is not a problem any more.

The principal components can be performed on Covariance or Correlation matrix. If the variables are of same units and their variances do not differ much, a covariance matrix can be used. However the population correlation matrix is unknown, its estimate can be used. In this thesis, sample correlation matrix is used to compute sample principal components. Construction of principal components requires following steps,
\begin{enumerate}
\item Estimate the correlation matrix $\bs{A}$ of $\bs{X}$ as,
    \begin{equation}
        \corr{\bs{X}}=\left(\diag{\Sigma}\right)^{-\frac{1}{2}}\Sigma\left(\diag{\Sigma}\right)^{-\frac{1}{2}}
        \label{eq:corFormula}
    \end{equation}
    Using sample observation, equation-\ref{eq:corFormula} can be estimated as,
    \begin{equation}
        \mbf{A}=\corr{\bs{X}}=\left(\diag{\mbf{S}}\right)^{-\frac{1}{2}}\mbf{S}\left(\diag{\mbf{S}}\right)^{-\frac{1}{2}}
        \label{eq:corSampFormula}
    \end{equation}
    Where \mbf{S} is the sample estimate of covariance matrix \bs{\Sigma},
    \begin{equation}
    	\mbf{S}=\mbb{E} \left[ \left( \mathbf{X} - \mbb{E}[\mathbf{X}] \right) \left( \mathbf{X} - \mbb{E}[\mathbf{X}] \right)^{\textrm{T}} \right] 
    \end{equation}

\item Calculate eigenvalue and eigenvector of the correlation matrix obtained in equation-\ref{eq:corSampFormula}. An eigenvalue $\mbf{\Lambda}$ of a square matrix $\mbf{A}$ of rank $p$ is a diagonal matrix of order $p$ which satisfies, 

  \begin{equation}
    \mbf{AE}=\mbf{E\Lambda}
    \label{eq:eigenEqn}
  \end{equation}
  
where,

\begin{equation}
\bs{\Lambda}=\text{diag}(\vl{\lambda}{p})
\label{eq:egnMatrix}
\end{equation}

In PCA these eigenvalues are arranged in descending order, i.e. $\lambda_1 \ge \lambda_2 \ge \ldots \ge \lambda_p$ . For each eigenvalues there is an eigenvector. Let $\mbf{E}=(\vl{\mbf{v}}{p})$ be the matrix of eigenvector so that the correlation matrix $\mbf{A}$ can be decomposed and expressed as,

\begin{equation}
  \mbf{A}=\mbf{E}\bs{\Lambda} \mbf{E}^{-1}=\mbf{E}\bs{\Lambda}\mbf{E}^T
  \label{eq:egnDecomp}
\end{equation}

Equivalently, $|\mbf{A}-\lambda_i \mbf{I}_n|\mbf{E}=0$ which can only be realized if $\mbf{A}-\lambda_i \mbf{I}_n$ is singular, i.e.,

  \begin{equation}
    |\mbf{A}-\lambda_i \mbf{I}_n|=0
    \label{eq:chrEqn}
  \end{equation}
  
Equation-\ref{eq:chrEqn} is called the characteristic equation where, $\mbf{A}$ is the correlation matrix obtained from equation-\ref{eq:corSampFormula}.  The root of the equation is called eigenvalues (\cite{seber2008matrix}) and the vector $\mbf{E}_i$ is called eigenvector corresponding to the eigenvalue $\lambda_i$. The eigenvector obtained from equation-\ref{eq:eigenEqn} are then normalized, i.e. $||\mbf{E}_i||^2=1$. 

\item Since, the variation explained in data are accumulated in first few  principal components, only $k$ eigenvalues are considered. The corresponding eigenvectors of those eigenvalues is called projection matrix. The projection matrix is,

\begin{equation}
  \mbf{P}=\begin{pmatrix} \mbf{E}_1^T & \mbf{E}_2^T & \ldots & \mbf{E}_k^T\end{pmatrix}^T
  \label{eq:projMatrix}
\end{equation}

The projection matrix in equation-\ref{eq:projMatrix} projects the data matrix into lower dimensional subspace $\mbf{Z}_i$. i.e.,

\begin{equation}
  \mbf{Z}=\mbf{PX}
  \label{eq:projComp}
\end{equation}

The column vectors of matrix $Z$ obtained from \ref{eq:projComp} are the orthogonal projections of data matrix $\mbf{X}$ into $k$ dimensional subspace. These components are the linear combination of the rows of matrix $\mbf{X}$ such that the most variance is explained by the first column vector of $\mbf{Z}$ and second one has less variance than the first one and so on. Here,
\begin{align*}
	\text{var}(\mbf{Z}_i)&=\lambda_i \text{ and }\\
	\text{cov}(\mbf{Z}_i\mbf{Z}_j)&=0\text{ for }i\ne j
\end{align*}
\end{enumerate}

\section{Principal Component Regression}
\label{sec:pcr}

The components of Principal Component Analysis (PCA) accumulate the variation in predictor variables on first few components. A linear regression fitted with only those components can give a similar results as the full linear model. However, \citet{jolliffe1982note} in his paper \citetitle{jolliffe1982note}, has given many examples taken from different papers of various fields where the components with low variance are also included in regression equation in order to explain most variation in the response variable. Following are the steps to perform Principal Component Regression. These steps are based on the paper \citetitle{yeniay2002comparison} by \cite{yeniay2002comparison}.

\begin{enumerate}
\item First principal components are obtained for \mbf{X} as explained in section-\ref{sec:pca}. The PCs obtained are orthogonal to each other.
\item Suppose $m$ PC which are supposed to influence the response are taken and a regression model is fitted as,
\begin{equation}
	\mbf{Y}=\mbf{Z_m}\alpha_m+\epsilon
\end{equation}

\item Here, $\alpha_m = \left(\mbf{Z}_m^T\mbf{Z}_m\right)^{-1}\mbf{Z}_m^T\mbf{Y}$ are the coefficients obtained from OLS methods. Using this alpha, one can obtain the estimate of $\bs{\beta}$ as,

\begin{equation}
	\bs{\hat{\beta}}_\text{PCR}=\bs{P}\left(\bs{P}^T\bs{X}^T\bs{XP}\right)^{-1}\bs{P}^T\bs{X}^T\bs{Y}
\end{equation}
Here, \bs{P} is a projection matrix defined in equation-\ref{eq:projMatrix}.
\end{enumerate}
Since, PCR includes only $m$ components, the estimate obtained are biased. ;The number of components $m$ can be chosen by cross-validation the prediction mean squared error (RMSEP). If all the components are included in the model, estimates obtained from PCR, i.e. $\bs{\beta_\text{PCR}}$ are identical to the estimates of OLS ($\bs{\beta}_\text{OLS}$).

\section{Partial Least Square Regression}
\label{sec:pls}

Partial Least Square Regression (PLS) is relatively new method and it can be used for both univariate and multivariate regression. It constructs a new set of variables called latent variable (or factor or components) from the linear combination of predictor variables $\vl{X}{n}$ (\cite{garthwaite1994interpretation}) as in the case of principal components, however PCR construct components (factors) maximizing the variation of data matrix$(X)$ while PLS construct them using the variation in both $X$ and $Y$ (\cite{yeniay2002comparison}). The intention of PLS is to create latent variables (components) that capture most of the information in the $X$ variables that is useful for predicting $\vl{Y}{p}$, while reducing the dimension of the regression problem by using fewer components than the number of X-variables (\cite{garthwaite1994interpretation}). Partial least square regression can be performed using following steps. These steps are adapted from the paper \citetitle{wold2001pls} from \citet{wold2001pls}. The $X$ and $Y$ matrices are column centered for the ease of computation.

\begin{enumerate}
\item{
PLS estimates the latent variables also called X-scores denoted by $t_a, (a~=~1,2,\ldots,A)$, where $A$ is the number of Components a model has considered. These X-scores are used to predict both X and Y, i.e. both X and Y are assumed to be modeled by the same latent variable. The X-scores are estimated as linear combination of original variables with the coefficients $W(w_{ka})$ as in equation-\ref{eq:PLSxScore}, i.e,
\begin{eqnarray}
\label{eq:PLSxScore}
    t_{ia}=\sum_{k=1}^{p}{W^*_{ka}X_{ik}} & (\bs{T}=\bs{X}\bs{W^*})
\end{eqnarray}

Where, $\bs{W}^*$ is a vector of weights $w^*_a$ of \bs{X}. It is obtained as in equation-\ref{eq:PLSWeight} below as a normalized coefficients obtained on regressing $X$ on a column of $Y$.

\begin{equation}
\label{eq:PLSWeight}
\bs{W}^*=\frac{\bs{X}^t\bs{y}^{(i)}}{\|\bs{X}^t\bs{y}^{(i)}\|}
\end{equation}

Here, $\bs{y}^{(i)}$ is any column of response matrix $\bs{Y}$.
}
\item{
The x-scores ($T$) are used to summarize \bs{X} as in the equation-\ref{eq:PLSxSumry}. Since the summary of \bs{X} explained most of the variations, the residuals (\bs{E}) are small.
\begin{eqnarray}
X_{ik} =\sum_a{t_{ia}P_{ak}+e_{ik}}; & (\bs{X}=\bs{T}\bs{P'}+\bs{E}) \label{eq:PLSxSumry} 
\end{eqnarray}

A similar setup can be used to have the summary for Y-matrix as in equation-\ref{eq:PLSySumry},
\begin{eqnarray}
Y_{im} =\sum_a{u_{ia}q_{am}+g_{im}}; & (\bs{Y}=\bs{U}\bs{Q'}+\bs{G}) \label{eq:PLSySumry}
\end{eqnarray}

where, $\bs{U}=\bs{YQ}$ and $\bs{Q}=\bs{T}^t\bs{Y}$
}
\item{
The X-scores ($\bs{T_\circ}$) are also good predictor of \bs{Y}, i.e.,
\begin{eqnarray}
\label{eq:PLSyPred}
y_{im}=\sum_a{q_{ma}t_{ia}+f_{im}} & (\bs{Y}=\bs{TC}^t+\bs{F})
\end{eqnarray}
Here, \bs{F} is the deviation between the observed and modeled response.
}
\item{
\textbf{Coefficients Estimates:}\\
Equation(\ref{eq:PLSyPred}) can also be written as,
\begin{align*}
y_{im}&=\sum_a{q_{ma}\sum_k{w_{ka}^*x_{ik}}}+f_{im}\\
&=\sum_k{b_{mk}x_{ik}}+f_{im}
\end{align*}
In matrix notation this can be written as,
\begin{equation}
\bs{Y}=\bs{XW}^*\bs{C}^t+\bs{F}=\bs{XB}+\bs{F}
\end{equation}
Thus, the estimates of PLS coefficients are obtained as,
\begin{align}
\hat{b}_{mk}&=\sum_a{q_{ma}w_{ka}^*}\\
i.e., \bs{B}_\text{PLS} &= \bs{W}^*\bs{C}^t
\end{align}
}
\end{enumerate}
Above process is repeated for each components ($a$), the matrix \bs{X} and \bs{Y} are ``deflated'' by subtracting their best summaries ($\bs{TP}^t$ for \bs{X} and $\bs{QC}^t$ for \bs{Y}). The Residuals obtained are used as new \bs{X} and \bs{Y} in the computation process for new component. However, the deflation of \bs{Y} is not necessary since the result is equivalent with or without the deflation (\cite[p.~5]{wold2001pls}).

Various algorithm exist to perform PLS regression among which NIPLS and SIMPLS are in fashion. This thesis has opted NIPLS (Nonlinear Iterative Partial Least Square) regression which is performed by \texttt{oscores} method of \texttt{pls} package in R. In the algorithm, the first weight vector ($\bs{w}_1$) is the first eigenvector of the combined variance-covariance matrix $\bs{X}^t\bs{YY}^t\bs{X}$ and the following weight vectors are computed using the deflated version. Similarly, the first score vector ($\bs{t}_1$) is computed as the first eigenvector of $\bs{XX}^t\bs{YY}^t$ and the following x-scores uses the deflated version of the matrices.

\section{Ridge Regression}
\label{sec:ridgeRegression}
When the minimum eigenvalue of $\bs{X}^t\bs{X}$ matrix is very much smaller than unity (i.e. $\lambda_\text{min}<<1$), the least square estimate obtained from equation-\ref{eq:estOLS} are larger than average (\cite{marquardt1975ridge}).
Estimates based on $\left[\bs{X}^t\bs{X}+\lambda\bs{I}_p\right], \lambda \ge 0$ rather than $\bs{X}^t\bs{X}$ can solve these problems. A.E. Hoel first suggests that to control instability of the least square estimate, on the condition above, can be;
\begin{align}
\hat{\bs{\beta}}^*_\text{ridge}&=\left[\bs{X}^t\bs{X}+\lambda\bs{I}\right]^{-1}\bs{X}^t\bs{Y};\; \lambda \ge 0 \nonumber\\
&=\bs{WX}^t\bs{Y}
\label{eq:ridgeBeta}
\end{align}
The analysis build around equation-\ref{eq:ridgeBeta} is called ``ridge equation''. The relationship of ridge estimate with ordinary least square is,
\begin{align}
\bs{\beta}_\text{ridge}&=\left[\bs{I}_p+\lambda\left(\bs{X}^t\bs{X}\right)^{-1}\right]^{-1}\hat{\beta}_\text{OLS} \nonumber \\
&=\bs{Z}\hat{\bs{\beta}}_\text{OLS}
\end{align}
Here, as $\lambda \rightarrow 0, \hat{\beta}_\text{ridge}=\hat{\beta}_\text{OLS}$ and $\lambda\rightarrow \infty, \hat{\beta}_\text{ridge}=0$
Further, the hat matrix for Ridge regression is given as,
\begin{equation}
\bs{H}_\text{ridge}=\bs{X}\left(\bs{X}^t\bs{X}+\lambda\bs{I}\right)^{-1}\bs{X}^t
\end{equation}
All the theory behind Ridge Regression described above are cited from \citetitle{hoerl1970ridge} by \citet{hoerl1970ridge}.

\section{Comparison Criteria}
\label{sec:compCriteria}
After fitting models with various methods, it becomes necessary to test their validity for their results to be trusted. Models react differently for the new information during prediction as the quality of model highly depends on their estimates. Since the purpose of this thesis is to compare different models, the basis for their comparison are set as their \begin{inlinelist}\item Goodness of fit and \item Predictability \end{inlinelist}.

\subsection{Goodness of fit}
\label{ssec:goodnessFit}
A model is assumed to follow some hypothetical state of being ideal. Setting up this state as null hypothesis ($H_\circ$), in many situations, the test of goodness of fit for a model construct an alternative hypothesis simply stating that the model gives little or no information about the distribution of the data. However in other situation, such as testing for no effect of some specific variable in the model, rejection of $H_\circ$ indicate that the variable is useful in the model (\cite[p.~1]{d1986goodness}). A goodness of fit for a model depends on many aspects such as,
\begin{description}
\item [Residual obtained after the model fit] \hfill\\
Residuals obtained from the fitted model are assumed to be random and normal considering that no useful information are still content on them.
\item[Outlier] \hfill\\
Outliers can distort the analysis toward unintentional direction creating false estimates. Models without such outliers are considered better.
\item [Variance explained by the model] \hfill\\
The variance explained by the model is generally measured by $R^2$ or $R^2 \text{ adj}$ in linear models. More the variation contained in the data is explained by the model, better the model is considered. In the case of PLS and PCR, the residuals contains very little information left on the ignored components.
\item [Relative value of Information Criteria such as AIC and BIC] \hfill\\
AIC (Akaike information criterion) and BIC (Bayesian information criterion or Schwarz criterion) measures relative quality of models. Although, it is not an absolute measure of the model quality, it helps to select a better model among others. AIC is defined as in equation - \ref{eq:aic} which is free from the ambiguities present in the conventional hypothesis testing system (\cite{akaike1974new}).
\begin{equation}
\label{eq:aic}
\text{AIC} = (-2)\log(\mathcal{L}) + 2(k)
\end{equation}
where, $\mathcal{L} =$ maximum likelihood and $k =$ number of independently adjusted parameters  within the model
For least square case, above formula resembles to equation - \ref{eq:aicFormula} (\cite{hu2007akaike}).
\end{description}

\subsection{Predictability}
\label{ssec:predictability}
Prediction is highly influenced by the model in used. So, prediction strongly depends on the estimates of a model. False and unstable estimate makes the prediction  poor and unreliable. On one side, providing more information (variable) can well train the model resulting more precise prediction. On the other hand, over-fitting, which attempts to explain idiosyncrasies in the data, leads to model complexity reducing the predictive power of a model. In the case of PLS and PCR, adding more components results in including noise in the model. 
<<mdlFitCriteriaPlot, child="mdlFitCriteriaPlot.Rnw">>=
@
The relationship between the model complexity and the prediction error is presented in figure-\ref{fig:mdlErrorPlt} with the case of under-fitting and over-fitting of a model.

Furthermore, a model exhibits an \textit{external validity} if it closely predicts the observations that were not used to fit the model parameters (\cite[p.~72]{lattin2003analyzing}). An over-fitted model fails to perform well for those observation that are not included during model parameter estimation. The dataset in this thesis is divided into two parts. The first part includes the observations from Jan 2000 to December 2012 and the second one includes observation onward till November 2014. A cross-validation  approach is utilized on the first set of observation to train the model. The model is used to predict the exchange rate of NOK per Euro from the predictors of the second set of observations. Figure - \ref{fig:calcProcedure} shows the procedure adopted for prediction in this thesis.

\begin{figure}[ht]
\centering
    \includestandalone[width=\linewidth]{include/calcProcedure}
    \caption[Procedure adopted in the thesis]{Procedure adopted in the thesis for model comparison. A cross-validation technique is used to validate the trained dataset. The trained model is used to predict the test response from with prediction errors are obtained.}
    \label{fig:calcProcedure}
\end{figure}

\subsubsection{Cross-Validation}
\label{sssec:crossValidation}
There are various cross-validation techniques among which two are described below;
\begin{description}
\item[ K-Fold Cross-validation:]\hfill \\
The dataset are split into $k$ equal parts. For each $i=1,2,\ldots, k$, a model is fitted leaving out the $i^\text{th}$ portion. A prediction error is calculated for this model. The process is repeated for all $i$. The prediction error for K-fold cross validation is obtained by averaging the prediction error of each of the model fitted.
\item[ Leave-one-out cross validation:]\hfill \\
This is a special case of $k-$ fold cross-validation where $k=n$ (number of observation), i.e, each time one observation is removed and the model is fitted.
\end{description}

\subsubsection{Prediction Error}
\label{sssec:predErr}
Prediction of a model becomes precise if the error is minimum. Models can be compared according to their predictability. Understanding of different measures of prediction error is necessary to acknowledge their predictability and eventually perform model comparison.
\begin{description}
\item[Root Mean Square Error (RMSE)]\hfill\\
RMSE is the measure of how well the model fit the data.
\begin{equation}
\text{RMSE}=\sqrt{\frac{1}{n}\sum_{i=1}^n{\left(y_i-\hat{y}_i\right)^2}}
\end{equation}
Where, \\
$\hat{y}_i$ are predicted values for $y_i$ and\\
$n$ is the number of observation

\item[Root Mean Square Error of Cross-Validation (RMSECV)]\hfill\\
RMSECV gives the models ability to predict new samples that were not present in the model during calibration. It is obtained as,
\begin{align}
\text{RMSECV} &=\sqrt{\frac{\text{PRESS}}{n}} \label{eq:rmsecv}\\
\text{Where, } & \nonumber \\
\text{PRESS} &=\sum_{i=1}^n{\left(y_i-\hat{y}_{(i)}\right)^2} \label{eq:press}
\end{align}
\end{description}
In the special case of leave one out cross validation, $i$ represents each sample.

\subsubsection{R-squared for Prediction}
\label{sssec:rseqPred}
R-squared for prediction is analogs to the R-sq in the case of model estimation. In the case of cross-validation, it is also denoted by $Q^2$. It is obtained by subtracting the ratio of PRESS obtained from equation-\ref{eq:press} to total sum of square from one. i.e,
\begin{equation}
\label{eq:rsqpred}
R^2_{CV}=Q^2=1-\frac{\text{PRESS}}{\text{TSS}} = 1-\frac{\sum_{i=1}^n{\left(y_i-\hat{y}_{(i)}\right)^2}}{\sum_{i=1}^n{\left(y_i-\bar{y}\right)^2}}
\end{equation}
Here, $Q^2<1$ and when prediction is very bad, PRESS may exceed TSS resulting negative value suggesting that the average value is better than the prediction using the model. 
