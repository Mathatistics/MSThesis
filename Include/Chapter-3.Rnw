% !Rnw root = ../Main.Rnw

\chapter{Models and Methods}
\label{chap:modelsmethods}

\section{A statistical Model}
\label{sec:statModel}

A statistical model describes the relationship between a cause and its effect. A vector $\mbf{y}$ contains $n$ number of responses. $\mbf{X}$ be a $n\times p$ matrix whose columns are independent variables and each of them have $n$ observations. These variables in $\mbf{X}$ can affect $\mbf{y}$ so, the relationship between $\mbf{X}$ and $\mbf{y}$ can be written in a functional form as, 

\begin{equation}
  \label{eq:functionalForm}
  \mbf{y}=f(\mbf{X})+\mbf{\epsilon}
\end{equation}

where, $\mbf{\epsilon}$ is a vector of unknown errors usually referred as `white noise' when dealing with time-series data which is assumed to have zero mean and constant variance.

\section{Linear Regression Model}
\label{sec:linRegModel}

The linear regression model with a single response $(\mbf{Y})$ and $p$ predictor variable $\vl{x}{p}$ has form,

\begin{equation}
    \underbracket[0.5pt][5pt]{\mbf{Y}}_{\text{Response}}=\underbracket[0.5pt][5pt]{\beta_0+\beta_1x_1+\beta_2x_2+\ldots+\beta_px_p}_{\text{Mean Response explained by predictors only}}+\underbracket[0.5pt][5pt]{\epsilon}_{\text{Error Term}}
    \label{eq:linearRegModel}
\end{equation}

The model - \ref{eq:linearRegModel} is linear function of $p$ unknown parameters $\vl{\beta}{p}$ which is generally refered as regression coefficients. In matrix notation, eqn (\ref{eq:linearRegModel}) becomes,

\begin{equation}
  \label{eq:linRegMatModel}
  \underset{n\times 1}{\mbf{Y}}=\underset{n\times(p+1)}{\mbf{X}}\underset{(p+1)\times 1}{\bs{\beta}}+\underset{n\times 1}{\bs{\epsilon}}
\end{equation}

The residual term $\bs{\epsilon}$ requires following assumptions,

\begin{align}
  \text{\bf{E}}(\bs{\epsilon})&=\mbf{0} \notag \\
  \text{and } \text{\bf{cov}}\bs{(\epsilon)}&=E(\bs{\epsilon\epsilon^t})=\sigma^2\mbf{I}
  \label{eq:linMatModelAssumption}
\end{align}

\subsection{Least Square Estimation}
\label{ssec:lstSqEstimation}

The unknown parameter $\bs{\beta}$ in (\ref{eq:linRegMatModel}) is obtained by minimizing the sum of square of residuals (\cite{yeniay2002comparison}), 
The sum of square of residuals is,

\begin{equation}
  \bs{\epsilon}^{t}\bs{\epsilon}=(\mbf{Y-X}\bs{\beta})^{t}(\mbf{Y-X}\bs{\beta})
  \label{eq:errorSq}
\end{equation}

On minimizing eq - \ref{eq:errorSq}, we get the OLS estimate of $\bs{\beta}$ as,

\begin{equation}
  \hat{\bs{\beta}}_{OLS}=(\mbf{X}^{t}\mbf{X})^{-1}\mbf{X}^{t}\mbf{Y}
  \label{eq:estOLS}
\end{equation}

Under the assumption in eq-\ref{eq:linMatModelAssumption}, the OLS estimate obtained from eq-\ref{eq:estOLS} is best linear unbiased estimator of $\beta$ (\cite{wooldridge2012introductory}).

\subsection{Prediction}
\label{ssec:pred}

Using $\hat{\bs{\beta}}$ obtained in eq-\ref{eq:estOLS}, following two matrices can be obtained,

\begin{subequations}
\begin{align}
  \text{Predicted Values:}\hat{\mbf{Y}}   &=\mbf{X}\hat{\bs{\beta}}=\mbf{X(X^tX)^{-1}X^tY} \label{eq:predEquation}\\
  \text{Residuals:} \hat{\bs{\epsilon}}&=\mbf{Y}-\hat{\mbf{Y}} =[\mbf{I-X(X^tX)^{-1}X^t}]\mbf{Y} \label{eq:OLSResid}
\end{align}
\end{subequations}

Here eq-\ref{eq:predEquation} gives predicted values of $\mbf{Y}$ which on subtracting from the observed value give the error terms as is presented in eq-\ref{eq:OLSResid}. Eq-\ref{eq:predEquation} can also be written as, 

\begin{equation}
  \hat{\mbf{Y}} =\mbf{X}\hat{\bs{\beta}}=\mbf{HY} \label{eq:predHatEquation}
\end{equation}

Here, $\mbf{H}$ is called Hat matrix and is the orthogonal projection of $y$ onto the space spanned by $X$.



\section{Principal Component Analysis}
\label{sec:pca}

The purpose of PCA is to express the information in $\bs{X}=(\vl{X}{p})$ by a less number of variables $\hat{Z}=(\vl{Z}{q}); q<p$ called principal components of X (\cite{martens1992multivariate}). These principal components are orthogonal and linearly independent. Since they are computed from the linear combinations of $\bs{X}$ variables, the variation in these variables are compressed in first few principal components. In other words, the first principal components is the direction along which the $\bs{X}$ variables have the largest variance (\cite{massart1998handbook}). In this situation, the multicollinearity in $\bs{X}$ is not a problem any more.

The principal components can be performed on Covariance matrix or in Correlation matrix. If the variables are of same units and their variances do not differ much, we can use the covariance matrix. In this thesis, the correlation matrix is used to compute principal components since, the $X$ variables are of different units and their variations have large difference. Construction of principal components requires following steps,
\begin{enumerate}
\item Calculate the correlation of data matrix $\bs{X}$

\begin{equation}
    \corr{\bs{X}}=\left(\diag{\Sigma}\right)^{-\frac{1}{2}}\Sigma\left(\diag{\Sigma}\right)^{-\frac{1}{2}}
    \label{eq:corFormula}
\end{equation}
Where,
\begin{equation}
	\Sigma=\mathrm{E} \left[ \left( \mathbf{X} - \mathrm{E}[\mathbf{X}] \right) \left( \mathbf{X} - \mathrm{E}[\mathbf{X}] \right)^{\rm T} \right] 
\end{equation}

\item Calculate eigenvalue and eigenvector of the correlation matrix obtained in eq-\ref{eq:corFormula}. An eigenvalue $\Lambda$ of a square matrix $\mbf{A}$ of rank $p$ is a diagonal matrix of order p which satisfies, 

  \begin{equation}
    \mbf{AE}=\mbf{E\Lambda}
    \label{eq:eigenEqn}
  \end{equation}
  
Each column of matrix $\mbf{E}$ is called an eigenvector corresponding to the eigenvalues $\lambda_i$, an $i^\text{th}$ element of diagonal matrix $\Lambda$. Equivalently, $|\mbf{A}-\lambda_i \mbf{I}_n|\mbf{E}=0$ which can only be realized if $|\mbf{A}-\lambda_i \mbf{I}_n|$ is singular, i.e.,

  \begin{equation}
    |\mbf{A}-\lambda_i \mbf{I}_n|=0
    \label{eq:chrEqn}
  \end{equation}
  
Eq-\ref{eq:chrEqn} is called the characteristic equation where, $\mbf{A}$ is the correlation matrix obtained from eq-\ref{eq:corFormula}.  The root of the equation is called eigenvalues (\cite{seber2008matrix}) and the vector $\mbf{E}_i$ is called eigenvector corresponding to the eigenvalue $\lambda_i$.

\item The eigenvector obtaind from eq-\ref{eq:eigenEqn} is normalized, i.e. $||\mbf{E}_i||^2=1$. In matrix form the eq-\ref{eq:eigenEqn} can be written as,

\begin{equation}
  \mbf{AE}=\mbf{E\Lambda}
\end{equation}

where,

\begin{equation}
\begin{bmatrix}
  \lambda_1 & 0 & \ldots & 0 \\
  0 & \lambda_2 & \ldots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & \ldots & \lambda_p
\end{bmatrix}
\label{eq:egnMatrix}
\end{equation}

The matrix in eq-\ref{eq:egnMatrix} has eigenvalues of matrix $A$ in its diagonal. In PCA these eigenvalues are arranged in descending order. i.e. $\lambda_1 \ge \lambda_2 \ge \ldots \ge \lambda_p$ for eigenvectors $\mbf{E}=(\vl{\mbf{v}}{p})$ The eigenvalue decomposition of the matrix $\mbf{A}$ is then written as,

\begin{equation}
  \mbf{A}=\mbf{E}\bs{\Lambda} \mbf{E}^{-1}=\mbf{E}\bs{\Lambda}\mbf{E}^T
  \label{eq:egnDecomp}
\end{equation}

Since, $\mbf{E}$ is a orthogonal matrix generated from a symmetric and positive definite matrix $\mbf{A}$

\item Since, the variation explained in data are accumulated in first few  principal components, only $k$ eigenvalues in descending order are considered while computing it. The corresponding eigenvectors of those eigenvalues is called projection matrix. The projection matrix is,

\begin{equation}
  \mbf{P}=\begin{pmatrix} \mbf{E}_1^T & \mbf{E}_k^T & \ldots & \mbf{E}_k^T\end{pmatrix}^T
  \label{eq:projMatrix}
\end{equation}

The projection matrix in eq-\ref{eq:projMatrix} projects the datamatrix into lower dimensional subspace $\mbf{Z}_i$. i.e.,

\begin{equation}
  \mbf{Z}=\mbf{PX}
  \label{eq:projComp}
\end{equation}

The columns vectors of matrix $Z$ obtained from \ref{eq:projComp} are the orthogonal projections of data matrix $\mbf{X}$ into $k$ dimensional subspace. These components are the linear combination of the rows of matrix $\mbf{X}$ such that the most variance is explained by the first column vector of $\mbf{Z}$ and second one has less variance than the first one and so on. Here,
\begin{align*}
	\text{var}(\mbf{Z}_i)&=\lambda_i \text{ and }\\
	\text{cov}(\mbf{Z}_i\mbf{Z}_j)&=0\text{ for }i\ne j
\end{align*}

\end{enumerate}

\section{Principal Component Regression}
\label{sec:pcr}

The components obtained from Principal Component Analysis contains the variation in predictor variables are accumulated on first few components. Since these components contains the variation of data, a linear regression is fitted with only those components. However, \citet{jolliffe1982note} in his paper \citetitle{jolliffe1982note}, has given many examples taken from different papers of various fields where the components with low variance are also included in regression equation in order to explain most variation in the response variable. Following are the steps to perform Principal Component Regression. These steps are based on the paper \citetitle{yeniay2002comparison} by \cite{yeniay2002comparison}.

\begin{enumerate}
\item First principal components are obtained for \mbf{X} as explained in section-\ref{sec:pca}. The PCs obtained are orthogonal to each other.
\item Suppose $m$ PC which are supposed to influence the response are taken and a regression model is fitted as,
\begin{equation}
	\mbf{Y}=\mbf{Z_m}\alpha_m+\epsilon
\end{equation}

\item Here, $\alpha_m = \left(\mbf{Z}_m^T\mbf{Z}_m\right)^{-1}\mbf{Z}_m^T\mbf{Y}$ are the coefficients obtained from OLS methods. Using this alpha, one can obtain the estimate of $\bs{\beta}$ as,

\begin{equation}
	\bs{\hat{\beta}}_\text{PCR}=\left(\bs{P}^T\bs{X}^T\bs{XP}\right)^{-1}\bs{P}^T\bs{X}^T\bs{Y}
\end{equation}
Here, \bs{P} is a projection matrix defined in equation-\ref{eq:projMatrix}.
\end{enumerate}
Since, PCR includes only $m$ componets, the estimate obtained are biased. ;The number of components $m$ can be chosen by cross-validation the prediction mean squared error (RMSEP). If all the components are included in the model, estimates obtained from PCR, i.e. $\bs{\beta_\text{PCR}}$ are identical to the estimates of OLS ($\bs{\beta}_\text{OLS}$).

\section{Partial Least Square Regression}
\label{sec:pls}

Partial Least Square Regession (PLS) is relatively new methods and it can be used for both univariate and multivariate regression. It constructs a new set of variables called latent variable (or factor or components) from the linear combination of predictor variables $\vl{X}{n}$ (\cite{garthwaite1994interpretation}) as in the case of principal components, however PCR construct components (factors) maximizing the variation of data matrix$(X)$ while PLS construct them using the variation in both $X$ and $Y$ (\cite{yeniay2002comparison}). The intension of PLS is to create latent variables (components) that caputre most of the information in the $X$ variables that is useful for predicting $\vl{Y}{p}$, while reducing the dimensionality of the regression problem by using fewer components than the number of X-variables (\cite{garthwaite1994interpretation}). Partial least square regression can be performed using following steps. These steps are adapted from the paper \citetitle{wold2001pls} from \citet{wold2001pls}. The $X$ and $Y$ matrices are column centered for the ease of computation.

\begin{enumerate}
\item{
PLS estimates the latent variables also called X-scores denoted by $t_a, (a~=~1,2,\ldots,A)$, where $A$ is the number of Components a model has considered. These X-scores are used to predict both X and Y, i.e. both X and Y are assumed to be modelled by the same latent variable. The X-scores are estimated as linear combination of original variables with the coefficients $W(w_{ka})$ as in equation-\ref{eq:PLSxScore}, i.e,
\begin{eqnarray}
\label{eq:PLSxScore}
    t_{ia}=\sum_{k=1}^{p}{W^*_{ka}X_{ik}} & (\bs{T}=\bs{X}\bs{W^*})
\end{eqnarray}

Where, $\bs{W}^*$ is a vector of weights $w^*_a$ of \bs{X}. It is obtained as in equation-\ref{eq:PLSWeight} below as a normalized coefficients obtained on regressing $X$ on a column of $Y$.

\begin{equation}
\label{eq:PLSWeight}
\bs{W}^*=\frac{\bs{X}^t\bs{y}^{(i)}}{\|\bs{X}^t\bs{y}^{(i)}\|}
\end{equation}

Here, $\bs{y}^{(i)}$ is any column of response matrix $\bs{Y}$.
}
\item{
The x-scores ($T$) are used to summarize \bs{X} as in the equation-\ref{eq:PLSxSumry}. Since the summary of \bs{X} explained most of the variations, the residuals (\bs{E}) are small.
\begin{eqnarray}
X_{ik} =\sum_a{t_{ia}P_{ak}+e_{ik}}; & (\bs{X}=\bs{T}\bs{P'}+\bs{E}) \label{eq:PLSxSumry} 
\end{eqnarray}

A similar setup can be used to have the summary for Y-matrix as in equation-\ref{eq:PLSySumry},
\color{red}{
\begin{eqnarray}
Y_{im} =\sum_a{u_{ia}q_{am}+g_{im}}; & (\bs{Y}=\bs{U}\bs{Q'}+\bs{G}) \label{eq:PLSySumry}
\end{eqnarray}

where, $\bs{U}=\bs{YQ}$ and $\bs{Q}=\bs{T}^t\bs{Y}$
}}
\item{
The X-scores ($\bs{T_\circ}$) are also good predictor of \bs{Y}, i.e.,
\begin{eqnarray}
\label{eq:PLSyPred}
y_{im}=\sum_a{q_{ma}t_{ia}+f_{im}} & (\bs{Y}=\bs{TC}^t+\bs{F})
\end{eqnarray}
Here, \bs{F} is the deviation between the observed and modelled response.
}
\item{
\textbf{Coefficients Estimates:}\\
Equation(\ref{eq:PLSyPred}) can also be written as,
\begin{align*}
y_{im}&=\sum_a{q_{ma}\sum_k{w_{ka}^*x_{ik}}}+f_{im}\\
&=\sum_k{b_{mk}x_{ik}}+f_{im}
\end{align*}
In matrix notation this can be written as,
\begin{equation}
\bs{Y}=\bs{XW}^*\bs{C}^t+\bs{F}=\bs{XB}+\bs{F}
\end{equation}
Thus, the estimates of PLS coefficients are obtained as,
\begin{align}
b_{mk}&=\sum_a{q_{ma}w_{ka}^*}\\
i.e., \bs{B}_\text{PLS} &= \bs{W}^*\bs{C}^t
\end{align}
}
\end{enumerate}
Above process is repreated for each components ($a$), the matrix \bs{X} and \bs{Y} are ``deflated'' by subtracting their best summaries ($\bs{TP}^t$ for \bs{X} and $\bs{QC}^t$ for \bs{Y}). The Residuals obtained are used as new \bs{X} and \bs{Y} in the computation process for new component. However, the deflation of \bs{Y} is not necessary since the result is equivalent with or without the deflation (\cite[p.~5]{wold2001pls}).

Various algorithm exist to perform PLS regression among which NIPLS and SIMPLS are in fashion. This thesis has opted NIPLS (Nonlinear Iterative Partial Least Square) regression which is performed by \texttt{oscores} method of \texttt{pls} package in R. Appendix - \ref{...} shows the flowchart for the algorithm. In the algorithm, the first weight vector ($\bs{w}_1$) is the first eigenvector of the combined variance-covariance matrix $\bs{X}^t\bs{YY}^t\bs{X}$ and the following weight vectors are computed using the deflated version. Similarly, the first score vector ($\bs{t}_1$) is computed as the first eigenvector of $\bs{XX}^t\bs{YY}^t$ and the following x-scores uses the deflated version of the matrices.

\section{Ridge Regression}
\label{sec:ridgeRegression}
When the minimum eigenvalue of $\bs{X}^t\bs{X}$ matrix is very much smaller than unity (i.e. $\lambda_\text{min}<<1$), the least square estimate obtained from equation-\ref{eq:estOLS} are larger than average (\cite{marquardt1975ridge}).
Estimates based on $\left[\bs{X}^t\bs{X}+\lambda\bs{I}_p\right], \lambda \ge 0$ rather than $\bs{X}^t\bs{X}$ can solve these problems. A.E. Hoel first suggests that to control instibility of the least square estimate, on the condition above, can be;
\begin{align}
\hat{\bs{\beta}}^*_\text{ridge}&=\left[\bs{X}^t\bs{X}+\lambda\bs{I}\right]^{-1}\bs{X}^t\bs{Y};\; \lambda \ge 0 \nonumber\\
&=\bs{WX}^t\bs{Y}
\label{eq:ridgeBeta}
\end{align}
The analysis build around equation-\ref{eq:ridgeBeta} is called ``ridge equation''. The relationship of ridge estimate with ordinary least square is,
\begin{align}
\bs{\beta}_\text{ridge}&=\left[\bs{I}_p+\lambda\left(\bs{X}^t\bs{X}\right)^{-1}\right]^{-1}\hat{\beta}_\text{OLS} \nonumber \\
&=\bs{Z}\hat{\bs{\beta}}_\text{OLS}
\end{align}
Here, as $\lambda \rightarrow 0, \hat{\beta}_\text{ridge}=\hat{\beta}_\text{OLS}$ and $\lambda\rightarrow \infty, \hat{\beta}_\text{ridge}=0$
Further, the hat matrix for Ridge regression is given as,
\begin{equation}
\bs{H}_\text{ridge}=\bs{X}\left(\bs{X}^t\bs{X}+\lambda\bs{I}\right)^{-1}\bs{X}^t
\end{equation}
All the theory behind Ridge Regression described above are cited from \citetitle{hoerl1970ridge} by \citet{hoerl1970ridge}.

\section{Comparision Criteria}
After fitting models with various methods, it becomes necessary to test their validity for their results to be trusted. Models react differently for the new information during prediction as the quality of model hightly depends on their estimates. Since the purpose of this thesis is to compare different models on the quality of their prediction, models are compared on the basis of their \begin{inlinelist}\item Goodness of fit and \item Predictability \end{inlinelist}.

\subsection{Goodness of fit}
\label{ssec:goodnessFit}
A model is assumed to follow some hypothetical state of being ideal. Setting up this state as null hypothesis ($H_\circ$), in many situations, the test of goodness of fit for a model construct an alternative hypothesis simply stating that the model gives little or no information about the distribution of the data. However in other situation, such as testing for no effect of some specific variable in the model, rejection of $H_\circ$ indicate that the variable is useful in the model (\cite[p.~1]{d1986goodness}). A goodness of fit for a model depends on many aspects such as,
\begin{description}
\item [Residual obtained after the model fit] \hfill\\
Residuals obtained from the fitted model are assumed to be random and normal considering that no useful information are still content on them.
\item[Outlier] \hfill\\
Outliers can distort the analysis toward unintential direction creating false estimates. Models without such outliers are considered better.
\item [Variance explained by the model] \hfill\\
The variance explained by the model is generally measured by $R^2$ or $R^2 \text{ adj}$ in linear models. More the variation contained in the data is explained by the model, better the model is considered. In the case of PLS and PCR, the residuals contains very little information left on the ignored components.
\item [Relative value of Information Criteria such as AIC and BIC] \hfill\\
AIC (Akaike information criterion) and BIC (Bayesian information criterion or Schwarz criterion) measures relative quality of model. Although, it is not an absolute measure of the model quality, helps to select a better model among others. AIC is defined as in equation - \ref{eq:aic} which is free from the ambiguities present in the convenential hypothesis testing system (\cite{akaike1974new}).
\begin{equation}
\label{eq:aic}
\text{AIC} = (-2)\log(\mathcal{L}) + 2(k)
\end{equation}
where, $\mathcal{L} =$ maximum likelihood and $k =$ number of independently adjusted parameters  within the model
\end{description}

\subsection{Predictability}
\label{ssec:predictability}
Prediction is highly influenced by the model in used. So, prediction strongly depends on the estimates of a model. False and unstable estimates makes the prediction  poor and unreliable. On one side, providing more information (variable) can well train the model resulting more precise prediction. On the other hand, overfitting, which attempts to explain idiosyncrasies in the data, leads to model complexity reducing the predictive power of a model. In the case of PLS and PCR, adding more components results in including noise in the model. 
<<mdlFitCriteriaPlot, child="mdlFitCriteriaPlot.Rnw">>=
@
The relationship between the model complexity and the prediction error is presented in figure-\ref{fig:mdlErrorPlt} with the case of under-fitting and over-fitting of a model.

Furthermore, a model exhibits an \textit{external validity} if it closely predicts the observations that were not used to fit the model parameters (\cite[p.~72]{lattin2003analyzing}). An overfitted model model fails to perform well for those observation that are not included during model parameter estimation. The dataset in this thesis is divided into two parts. The first part includes the observations from Jan 2000 to December 2012 and the second one includes observation onward till November 2014. A cross-validation  approach is utilized on the first set of observation to train the model. The model is used to predict the the exchange rate of NOK per Euro from the predictors of the second set of observations. Figure - \ref{fig:calcProcedure} shows the procedure adopted for prediction in this thesis.

\begin{figure}[ht]
\centering
    \includestandalone[width=\linewidth]{include/calcProcedure}
    \caption{Procedure adopted in the thesis for model comparision.A cross-validation technique is used to validate the trained dataset. The trained model is used to predict the test response from with prediction errors are obtained.}
    \label{fig:calcProcedure}
\end{figure}

\subsubsection{Cross-Validation}
\label{sssec:crossValidation}
There are various cross-validation techniques among which two were described below;
\begin{description}
\item[ K-Fold Cross-validation:]\hfill \\
The dataset are splitted into $k$ equal parts. For each $i=1,2,\ldots, k$, a model is fitted leaving out the $i^\text{th}$ portion. A prediction error is calculated for this model. The process is repeated for all $i$. THe prediction error for K-fold cross validation is obtained by averaging the prediction error of each of the model fitted.
\item[ Leave-one-out cross validation:]\hfill \\
This is a special case of $k-$ fold cross-validation where $k=n$ (number of observation), i.e, each time one observation is removed and the model is fitted.
\end{description}

\subsubsection{Prediction Error}
\label{sssec:predErr}


\subsubsection{R-squared for Prediction}
\label{sssec:rseqPred}
