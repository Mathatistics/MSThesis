% !Rnw root = ../Main.Rnw

\chapter{Models and Methods}

\section{A statistical Model}
A statistical model describes the relationship between a cause and its effect. A vector $\mbf{y}$ contains $n$ number of responses. $\mbf{X}$ be a $n\times p$ matrix whose columns are independent variables and each of them have $n$ observations. These variables in $\mbf{X}$ can affect $\mbf{y}$ so, the relationship between $\mbf{X}$ and $\mbf{y}$ can be written in a functional form as, 

\begin{equation}
  \label{eq:functionalForm}
  \mbf{y}=f(\mbf{X})+\mbf{\epsilon}
\end{equation}

where, $\mbf{\epsilon}$ is a vector of unknown errors usually referred as `white noise' when dealing with time-series data which is assumed to have zero mean and constant variance.

\subsection{Linear Regression Model}
The linear regression model with a single response $(\mbf{Y})$ and $p$ predictor variable $\vl{x}{p}$ has form,
\begin{equation}
    \underbracket[0.5pt][5pt]{\mbf{Y}}_{\text{Response}}=\underbracket[0.5pt][5pt]{\beta_0+\beta_1x_1+\beta_2x_2+\ldots+\beta_px_p}_{\text{Mean Response explained by predictors only}}+\underbracket[0.5pt][5pt]{\epsilon}_{\text{Error Term}}
    \label{eq:linearRegModel}
\end{equation}
The model - \ref{eq:linearRegModel} is linear function of $p$ unknown parameters $\vl{\beta}{p}$ which is generally refered as regression coefficients. The error term $\epsilon$ are assumed to follow,
\begin{align}
  \mbf{E}(\epsilon_j)&=0 \notag \\
  \text{\bf{Var}}(\epsilon_j)&=\sigma^2 \text{(Constant)} \notag \\
  \text{\bf{Cov}}(\epsilon_j, \epsilon_k)&=0\;j\ne k
  \label{eq:linModelAssumption}
\end{align}
In matrix notation, eqn (\ref{eq:linearRegModel}) becomes (\cite{johnson2007applied}),
\begin{equation}
  \label{eq:linRegMatModel}
  \underset{n\times 1}{\mbf{Y}}=\underset{n\times(p+1)}{\mbf{X}}\underset{(p+1)\times 1}{\bs{\beta}}+\underset{n\times 1}{\bs{\epsilon}}
\end{equation}
The assumption in (\ref{eq:linModelAssumption}) becomes,
\begin{align}
  \text{\bf{E}}(\bs{\epsilon})&=\mbf{0} \notag \\
  \text{and } \text{\bf{cov}}\bs{(\epsilon)}&=E(\bs{\epsilon\epsilon^t})=\sigma^2\mbf{I}
  \label{eq:linMatModelAssumption}
\end{align}

\subsubsection{Least Square Estimation}
The unknown parameter $\bs{\beta}$ in (\ref{eq:linRegMatModel}) is obtained by minimizing the sum of square of residuals (\cite{yeniay2002comparison}), 
The sum of square of residuals is,

\begin{equation}
  \bs{\epsilon}^{t}\bs{\epsilon}=(\mbf{Y-X}\bs{\beta})^{t}(\mbf{Y-X}\bs{\beta})
  \label{eq:errorSq}
\end{equation}

On minimizing eq - \ref{eq:errorSq}, we get the OLS estimate of $\bs{\beta}$ as,

\begin{equation}
  \hat{\bs{\beta}}_{OLS}=(\mbf{X}^{t}\mbf{X})^{-1}\mbf{X}^{t}\mbf{Y}
  \label{eq:estOLS}
\end{equation}
Under the assumption in eq-\ref{eq:linMatModelAssumption}, the OLS estimate obtained from eq-\ref{eq:estOLS} is best linear unbiased estimator of $\beta$ (\cite{wooldridge2012introductory}).

\subsubsection{Prediction}
Using $\hat{\bs{\beta}}$ obtained in eq-\ref{eq:estOLS}, following two matrices can be obtained,
\begin{subequations}
\begin{align}
  \text{Predicted Values:}\hat{\mbf{Y}}   &=\mbf{X}\hat{\bs{\beta}}=\mbf{X(X^tX)^{-1}X^tY} \label{eq:predEquation}\\
  \text{Residuals:} \hat{\bs{\epsilon}}&=\mbf{Y}-\hat{\mbf{Y}} =[\mbf{I-X(X^tX)^{-1}X^t}]\mbf{Y} \label{eq:OLSResid}
\end{align}
\end{subequations}
Here eq-\ref{eq:predEquation} gives predicted values of $\mbf{Y}$ which on subtracting from the observed value give the error terms as is presented in eq-\ref{eq:OLSResid}. Eq-\ref{eq:predEquation} can also be written as, 
\begin{equation}
  \hat{\mbf{Y}} =\mbf{X}\hat{\bs{\beta}}=\mbf{HY} \label{eq:predHatEquation}
\end{equation}
Here, $\mbf{H}$ is called Hat matrix and is the orthogonal projection of $y$ onto the space spanned by $X$(\cite{faraway2004linear}).

\section{Principal Component Analysis}
The purpose of PCA is to express the information in $X=(\vl{X}{p})$ by a less number of variables $\hat{Z}=(\vl{Z}{q}); q<p$ called principal components of X (\cite{martens1992multivariate}). These principal components are orthogonal and linearly independent. Since they are computed from the linear combination of $X$ variables, the variation in these variables are compressed in first few principal components. In other words, the first principal components is the direction along which the $X$ variables have the largest variance (\cite{massart1998handbook}). In this situation, the multicollinearity in $X$ is not a problem any more.

The principal components can be performed on Covariance matrix or in Correlation matrix. If the variables are of same units and their variances do not differ much, we can use the covariance matrix. In this thesis, the correlation matrix is used to compute principal components since, the $X$ variables are of different units and their variations have large difference. Calculation of Principal Component Analysis requires following steps:
\begin{enumerate}
\item Calculate the correlation of data matrix $X$
  \begin{equation}
    \text{cor}(X)=\frac{\text{cov}(X,Y)}{S_xS_y}=\frac{\sum_{i=1}^n{(X_i-\bar{X})(Y_i-\bar{Y})}}{\sqrt{\sum_{i=1}^{n}{(X_i-\bar{X})^2}\sum_{i=1}^{n}{(Y_i-\bar{Y})^2}}}
    \label{eq:corFormula}
  \end{equation}

\item Calculate eigenvalue and eigenvector of the correlation matrix obtained in eq-\ref{eq:corFormula}.
An eigenvalue $\lambda_i; i=1,\ldots p$ of a square matrix $\mbf{A}$ of order $p$ is a scalar which satisfies, 
  \begin{equation}
    \mbf{AE}_i=\lambda_i \mbf{E}_i
    \label{eq:eigenEqn}
  \end{equation}
The vector $\mbf{E}_i$ is called eigenvector(\cite{harville2008matrix}).

It is equivalently written as, $|\mbf{A}-\lambda_i \mbf{I}_n|E=0$ which can only be realized if $|\mbf{A}-\lambda_i \mbf{I}_n|$ is singular, i.e.,
  \begin{equation}
    |\mbf{A}-\lambda_i \mbf{I}_n|=0
    \label{eq:chrEqn}
  \end{equation}
Eq-\ref{eq:chrEqn} is called the characteristic equation where, $\mbf{A}$ is the correlation matrix obtained from eq-\ref{eq:corFormula}.  The root of the equation is called eigenvalues (\cite{seber2008matrix}) and the vector $\mbf{E}_i$ is called eigenvector corresponding to the eigenvalue $\lambda_i$.

\item The eigenvector obtaind from eq-\ref{eq:eigenEqn} is normalized, i.e. $||\mbf{E}_i||^2=1$. In matrix form the eq-\ref{eq:eigenEqn} can be written as,
\begin{equation}
  \mbf{AE}=\mbf{E\Lambda}
\end{equation}
where,
\begin{equation}
\begin{bmatrix}
  \lambda_1 & 0 & \ldots & 0 \\
  0 & \lambda_2 & \ldots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & \ldots & \lambda_p
\end{bmatrix}
\label{eq:egnMatrix}
\end{equation}

The matrix in eq-\ref{eq:egnMatrix} has eigenvalues of matrix $A$ in its diagonal. In PCA these eigenvalues are arranged in descending order. i.e. $\lambda_1 \ge \lambda_2 \ge \ldots \ge \lambda_p$ and eigenvectors $\mbf{E}=(\vl{\mbf{v}}{p})$ The eigenvalue decomposition of the matrix $\mbf{A}$ is then written as,
\begin{equation}
  \mbf{A}=\mbf{E}\bs{\Lambda} \mbf{E}^{-1}=\mbf{E}\bs{\Lambda}\mbf{E}^T
  \label{eq:egnDecomp}
\end{equation}
Since, $\mbf{E}$ is a orthogonal matrix generated from a symmetric and positive definite matrix $\mbf{A}$

\item Since, the variation explained in data are accumulated in first few  principal components, only $k$ eigenvalues in descending order are considered while computing it. The corresponding eigenvectors of those eigenvalues is called projection matrix. The projection matrix is,
\begin{equation}
  \mbf{P}=\begin{pmatrix} \mbf{E}_1^T \\ \mbf{E}_k^T \\ \vdots \\ \mbf{E}_k^T\end{pmatrix}
  \label{eq:projMatrix}
\end{equation}
The projection matrix in eq-\ref{eq:projMatrix} projects the datamatrix into low dimensional subspace $\mbf{z}_i$. i.e.,
\begin{equation}
  \mbf{z}_i=\mbf{PX}_i
  \label{eq:projComp}
\end{equation}
The new vectors $\mbf{z}_i$ obtained from \ref{eq:projComp} are the orthogonal projections of data matrix $\mbf{X}$ into $k$ dimensional subspace. These components are the linear combination of the rows of matrix $\mbf{X}$ such that the most variance is explained by the first component $\mbf{z}_1$ and second component has less variance than the first one and so on. These components are also called scores.
\end{enumerate}

\section{Principal Component Regression}
The components obtained from Principal Component Analysis contains the variation of the data accumulated on first few components. Since these components contains the variation of data, a linear regression is fitted with only those components. For instance, if modeling with the first three components can explain over 95\% of variation, the dimension of data is remarkably reduced with excluding noise.
\section{Partial Least Square Regression}
\section{Ridge Regression}
\section{Comparision Criteria}
<<mdlFitCriteriaPlot, child="mdlFitCriteriaPlot.Rnw">>=
@

  \subsection{PRESS (Predicted Residual Aum of Squares)}
  \subsection{RMSEP (Root Mean Standard Error of Prediction)}
  \subsection{R-squared for Prediction}
  \subsection{Goodness of Fit}